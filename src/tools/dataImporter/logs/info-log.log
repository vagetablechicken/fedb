13:53:25.832 [INFO] [main] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - /data/abc/logs
13:54:54.231 [INFO] [main] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - /data/abc/logs
15:14:25.915 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:14:26.205 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2072)] - Shutting down the Mini HDFS Cluster
15:15:19.378 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:15:19.622 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2072)] - Shutting down the Mini HDFS Cluster
15:32:33.505 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:32:33.842 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2072)] - Shutting down the Mini HDFS Cluster
15:36:08.815 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:36:09.070 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2072)] - Shutting down the Mini HDFS Cluster
15:46:55.791 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:46:56.381 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
15:46:56.402 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
15:46:56.403 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
15:46:56.404 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
15:46:56.439 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
15:46:56.439 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
15:46:56.440 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
15:46:56.440 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
15:46:56.478 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:46:56.482 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
15:46:56.483 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
15:46:56.483 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
15:46:56.487 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
15:46:56.488 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 15:46:56
15:46:56.490 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
15:46:56.490 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:46:56.493 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
15:46:56.493 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
15:46:56.515 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
15:46:56.515 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
15:46:56.520 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
15:46:56.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
15:46:56.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
15:46:56.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
15:46:56.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
15:46:56.522 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
15:46:56.543 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GLOBAL serial map: bits=29 maxEntries=536870911
15:46:56.543 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - USER serial map: bits=24 maxEntries=16777215
15:46:56.543 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GROUP serial map: bits=24 maxEntries=16777215
15:46:56.544 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - XATTR serial map: bits=24 maxEntries=16777215
15:46:56.553 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
15:46:56.553 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:46:56.553 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
15:46:56.553 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
15:46:56.571 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
15:46:56.572 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
15:46:56.572 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
15:46:56.572 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
15:46:56.576 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
15:46:56.577 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
15:46:56.581 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
15:46:56.581 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:46:56.581 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
15:46:56.581 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
15:46:56.588 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
15:46:56.589 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
15:46:56.589 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
15:46:56.592 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
15:46:56.593 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
15:46:56.594 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
15:46:56.594 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:46:56.595 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
15:46:56.595 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
15:47:06.632 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:185)] - Allocated new BlockPoolId: BP-1624480315-192.168.131.243-1622533626623
15:47:06.645 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 has been successfully formatted.
15:47:06.646 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 has been successfully formatted.
15:47:06.669 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
15:47:06.669 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
15:47:06.761 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
15:47:06.761 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
15:47:06.770 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.getImageTxIdToRetain(NNStorageRetentionManager.java:203)] - Going to retain 1 images with txid >= 0
15:47:06.772 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1656)] - createNameNode []
15:47:11.992 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:378)] - Scheduled Metric snapshot period at 10 second(s).
15:47:11.992 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:191)] - NameNode metrics system started
15:47:12.022 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeUtils.getClientNamenodeAddress(NameNodeUtils.java:79)] - fs.defaultFS is hdfs://127.0.0.1:0
15:47:12.070 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4c2bb6e0] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
15:47:12.095 [INFO] [main] [org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1645)] - Starting Web-server for hdfs at: http://localhost:0
15:47:12.121 [INFO] [main] [org.eclipse.jetty.util.log.Log.initialized(Log.java:169)] - Logging initialized @16823ms to org.eclipse.jetty.util.log.Slf4jLog
15:47:17.263 [INFO] [main] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
15:47:17.269 [INFO] [main] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.namenode is not defined
15:47:17.275 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
15:47:17.277 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
15:47:17.278 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
15:47:17.311 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:103)] - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
15:47:17.311 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:819)] - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
15:47:17.326 [INFO] [main] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 50315
15:47:17.328 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
15:47:17.354 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
15:47:17.355 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
15:47:17.356 [INFO] [main] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 600000ms
15:47:17.369 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@413f69cc{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
15:47:17.544 [INFO] [main] [org.eclipse.jetty.util.TypeUtil.<clinit>(TypeUtil.java:201)] - JVM Runtime does not support Modules
15:47:17.556 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@74eb909f{hdfs,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-50315-_-any-4557641291049990951.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/hdfs}
15:47:17.566 [INFO] [main] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@2fd953a6{HTTP/1.1,[http/1.1]}{localhost:50315}
15:47:17.567 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @22269ms
15:47:17.575 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
15:47:17.584 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
15:47:17.584 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
15:47:17.585 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:47:17.586 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
15:47:17.586 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
15:47:17.586 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
15:47:17.586 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 15:47:17
15:47:17.586 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
15:47:17.586 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:47:17.587 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
15:47:17.587 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
15:47:17.602 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
15:47:17.602 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
15:47:17.602 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
15:47:17.603 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
15:47:17.604 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
15:47:17.604 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:47:17.604 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
15:47:17.604 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
15:47:17.605 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
15:47:17.606 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
15:47:17.606 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
15:47:17.606 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
15:47:17.606 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
15:47:17.606 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
15:47:17.607 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
15:47:17.607 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:47:17.607 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
15:47:17.607 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
15:47:17.636 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
15:47:17.637 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
15:47:17.638 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
15:47:17.638 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
15:47:17.638 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
15:47:17.638 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
15:47:17.638 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:47:17.639 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
15:47:17.639 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
15:47:22.650 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/in_use.lock acquired by nodename 766@4paradigmdeMacBook-Pro.local
15:47:27.657 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/in_use.lock acquired by nodename 766@4paradigmdeMacBook-Pro.local
15:47:27.663 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current
15:47:27.664 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current
15:47:27.664 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:733)] - No edit log streams selected.
15:47:27.665 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:799)] - Planning to load image: FSImageFile(file=/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
15:47:27.689 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:234)] - Loading 1 INodes.
15:47:27.694 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:246)] - Loaded FSImage in 0 seconds.
15:47:27.694 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:977)] - Loaded image for txid 0 from /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000
15:47:27.698 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1140)] - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
15:47:27.699 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1365)] - Starting log segment at 1
15:47:27.711 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameCache.initialized(NameCache.java:143)] - initialized with 0 entries 0 lookups
15:47:27.711 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:757)] - Finished loading FSImage in 10071 msecs
15:47:27.856 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:451)] - RPC server is binding to localhost:0
15:47:27.857 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:456)] - Enable NameNode state context:false
15:47:27.863 [INFO] [main] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
15:47:27.872 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
15:47:28.099 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:746)] - Clients are to use localhost:50389 to access this namenode/service.
15:47:28.101 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(FSNamesystem.java:5123)] - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
15:47:28.122 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:176)] - Number of blocks under construction: 0
15:47:28.130 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.initializeReplQueues(BlockManager.java:5069)] - initializing replication queues
15:47:28.130 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:400)] - STATE* Leaving safe mode after 0 secs
15:47:28.130 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:406)] - STATE* Network topology has 0 racks and 0 datanodes
15:47:28.131 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:408)] - STATE* UnderReplicatedBlocks has 0 blocks
15:47:28.135 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3657)] - Total number of blocks            = 0
15:47:28.136 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3658)] - Number of invalid blocks          = 0
15:47:28.136 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3659)] - Number of under-replicated blocks = 0
15:47:28.136 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3660)] - Number of  over-replicated blocks = 0
15:47:28.136 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3662)] - Number of blocks being written    = 0
15:47:28.136 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3665)] - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
15:47:28.160 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
15:47:28.160 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
15:47:28.162 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:852)] - NameNode RPC up at: localhost/127.0.0.1:50389
15:47:28.163 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1252)] - Starting services required for active state
15:47:28.163 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:799)] - Initializing quota with 4 thread(s)
15:47:28.168 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:808)] - Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
15:47:28.171 [INFO] [CacheReplicationMonitor(972224770)] [org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:160)] - Starting CacheReplicationMonitor with interval 30000 milliseconds
15:47:28.178 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1682)] - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1,[DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:47:28.236 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:47:28.244 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:47:28.264 [INFO] [Listener at localhost/50389] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:158)] - DataNode metrics system started (again)
15:47:28.268 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:47:28.271 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.BlockScanner.<init>(BlockScanner.java:195)] - Initialized block scanner with targetBytesPerSec 1048576
15:47:28.274 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:500)] - Configured hostname is 127.0.0.1
15:47:28.275 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:47:28.286 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1403)] - Starting DataNode with maxLockedMemory = 0
15:47:28.293 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:1151)] - Opened streaming server at /127.0.0.1:50390
15:47:28.295 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:78)] - Balancing bandwidth is 10485760 bytes/s
15:47:28.295 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:79)] - Number threads for balancing is 50
15:47:33.411 [INFO] [Listener at localhost/50389] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
15:47:33.412 [INFO] [Listener at localhost/50389] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.datanode is not defined
15:47:33.414 [INFO] [Listener at localhost/50389] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
15:47:33.415 [INFO] [Listener at localhost/50389] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
15:47:33.415 [INFO] [Listener at localhost/50389] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
15:47:33.418 [INFO] [Listener at localhost/50389] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 50430
15:47:33.418 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
15:47:33.419 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
15:47:33.419 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
15:47:33.420 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 660000ms
15:47:33.420 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@6c2f1700{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
15:47:33.528 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@23c650a3{datanode,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-50430-_-any-7269175836702019103.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/datanode}
15:47:33.529 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@5298dead{HTTP/1.1,[http/1.1]}{localhost:50430}
15:47:33.529 [INFO] [Listener at localhost/50389] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @38231ms
15:47:38.659 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.start(DatanodeHttpServer.java:256)] - Listening HTTP traffic on /127.0.0.1:50464
15:47:38.659 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4a699efa] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
15:47:38.660 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1431)] - dnUserName = 4paradigm
15:47:38.661 [INFO] [Listener at localhost/50389] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1432)] - supergroup = supergroup
15:47:38.678 [INFO] [Listener at localhost/50389] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
15:47:38.678 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
15:47:38.682 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:1037)] - Opened IPC server at /127.0.0.1:50468
15:47:38.692 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.refreshNamenodes(BlockPoolManager.java:149)] - Refresh request received for nameservices: null
15:47:38.693 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(BlockPoolManager.java:210)] - Starting BPOfferServices for nameservices: <default>
15:47:38.704 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)] - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:50389 starting to offer service
15:47:38.707 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
15:47:38.707 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
15:47:38.876 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:379)] - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:50389
15:47:38.878 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.getParallelVolumeLoadThreadsNum(DataStorage.java:354)] - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
15:47:39.161 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.168 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.168 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.274 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.276 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.276 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.381 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.381 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.382 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.488 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.489 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.490 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.592 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.592 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.593 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.698 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.700 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.700 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.805 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.806 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.806 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:39.912 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:39.913 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:39.913 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.018 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.019 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.019 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.125 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.126 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.127 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.228 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.229 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.230 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.335 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.336 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.336 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.441 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.442 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.442 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.548 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.549 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.549 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.654 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.655 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.655 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.760 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.760 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.761 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.866 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.867 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.867 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:40.973 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:40.974 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:40.974 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.080 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.080 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.080 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.186 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.188 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.188 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.291 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.292 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.292 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.398 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.399 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.399 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.505 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.505 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.506 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.608 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.608 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.609 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.713 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.714 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.715 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.816 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.817 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.817 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:41.921 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:41.922 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:41.922 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.029 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.029 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.030 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.135 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.136 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.136 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.242 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.243 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.245 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.349 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.350 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.350 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.456 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.457 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.457 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.562 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.563 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.563 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.668 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.669 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.670 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.775 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.776 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.776 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.879 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.880 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.880 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:42.983 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:42.984 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:42.984 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.087 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.088 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.089 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.192 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.193 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.193 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.297 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.298 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.298 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.404 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.404 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.405 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.507 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.508 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.508 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.613 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.614 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.614 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.717 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.718 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.718 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.823 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.824 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.824 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:43.885 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/in_use.lock acquired by nodename 766@4paradigmdeMacBook-Pro.local
15:47:43.887 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 is not formatted for namespace 999251445. Formatting...
15:47:43.888 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 
15:47:43.929 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:43.930 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:43.930 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.038 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.039 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.039 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.141 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.142 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.142 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.249 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.250 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.250 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.357 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.358 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.358 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.460 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.461 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.461 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.567 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.567 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.567 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.674 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.675 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.675 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.780 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.781 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.781 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.887 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.888 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.888 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:44.989 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:44.990 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:44.991 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.097 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.098 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.098 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.203 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.204 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.204 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.307 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.308 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.308 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.412 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.413 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.413 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.520 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.521 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.521 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.625 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.626 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.626 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.730 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.731 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.732 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.836 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.836 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.837 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:45.941 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:45.942 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:45.942 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.048 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.048 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.048 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.154 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.155 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.156 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.259 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.260 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.260 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.366 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.367 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.367 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.471 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.472 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.473 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.576 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.576 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.576 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.680 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.680 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.680 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.785 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.786 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.786 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.892 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:46.892 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:46.893 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:46.999 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.000 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.000 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.105 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.106 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.106 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.209 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.210 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.210 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.316 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.316 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.316 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.421 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.422 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.422 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.523 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.524 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.524 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.628 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.629 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.629 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.732 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.733 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.733 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.836 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.837 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.837 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:47.941 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:47.941 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:47.942 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.047 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.047 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.048 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.152 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.153 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.153 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.258 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.259 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.259 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.360 [INFO] [IPC Server handler 8 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.361 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.361 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.467 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.467 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.468 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.570 [INFO] [IPC Server handler 2 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.571 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.571 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.674 [INFO] [IPC Server handler 1 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.675 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.675 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.780 [INFO] [IPC Server handler 7 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.781 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.781 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.887 [INFO] [IPC Server handler 3 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.888 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.888 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:48.897 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/in_use.lock acquired by nodename 766@4paradigmdeMacBook-Pro.local
15:47:48.897 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 is not formatted for namespace 999251445. Formatting...
15:47:48.897 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755 for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 
15:47:48.919 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1624480315-192.168.131.243-1622533626623
15:47:48.920 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1624480315-192.168.131.243-1622533626623
15:47:48.920 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 and block pool id BP-1624480315-192.168.131.243-1622533626623 is not formatted. Formatting ...
15:47:48.920 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1624480315-192.168.131.243-1622533626623 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1624480315-192.168.131.243-1622533626623/current
15:47:48.937 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1624480315-192.168.131.243-1622533626623
15:47:48.937 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1624480315-192.168.131.243-1622533626623
15:47:48.937 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 and block pool id BP-1624480315-192.168.131.243-1622533626623 is not formatted. Formatting ...
15:47:48.937 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1624480315-192.168.131.243-1622533626623 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1624480315-192.168.131.243-1622533626623/current
15:47:48.938 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1751)] - Setting up storage: nsid=999251445;bpid=BP-1624480315-192.168.131.243-1622533626623;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=999251445;c=1622533626623;bpid=BP-1624480315-192.168.131.243-1622533626623;dnuuid=null
15:47:48.939 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.checkDatanodeUuid(DataNode.java:1549)] - Generated and persisted new Datanode UUID fe73600d-f62f-429d-b6fd-810885d31849
15:47:48.990 [INFO] [IPC Server handler 4 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:48.990 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:48.990 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:49.028 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e
15:47:49.029 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, StorageType: DISK
15:47:49.030 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755
15:47:49.031 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, StorageType: DISK
15:47:49.035 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader.initialize(MemoryMappableBlockLoader.java:48)] - Initializing cache loader: MemoryMappableBlockLoader.
15:47:49.040 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.registerMBean(FsDatasetImpl.java:2320)] - Registered FSDatasetState MBean
15:47:49.045 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:2833)] - Adding block pool BP-1624480315-192.168.131.243-1622533626623
15:47:49.045 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
15:47:49.046 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
15:47:49.077 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1624480315-192.168.131.243-1622533626623 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 31ms
15:47:49.077 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1624480315-192.168.131.243-1622533626623 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 30ms
15:47:49.077 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:431)] - Total time to scan all replicas for block pool BP-1624480315-192.168.131.243-1622533626623: 32ms
15:47:49.079 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
15:47:49.079 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
15:47:49.079 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1624480315-192.168.131.243-1622533626623/current/replicas doesn't exist 
15:47:49.079 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1624480315-192.168.131.243-1622533626623/current/replicas doesn't exist 
15:47:49.092 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 2ms
15:47:49.092 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 14ms
15:47:49.092 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getAllVolumesMap(FsVolumeList.java:225)] - Total time to add all replicas to map for block pool BP-1624480315-192.168.131.243-1622533626623: 14ms
15:47:49.092 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:47:49.097 [INFO] [IPC Server handler 9 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:49.098 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:47:49.098 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:47:49.102 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:47:49.103 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:47:49.103 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:47:49.104 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:47:49.105 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1624480315-192.168.131.243-1622533626623 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:47:49.108 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755): finished scanning block pool BP-1624480315-192.168.131.243-1622533626623
15:47:49.108 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e): finished scanning block pool BP-1624480315-192.168.131.243-1622533626623
15:47:49.121 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755): no suitable block pools found to scan.  Waiting 1814399983 ms.
15:47:49.122 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e): no suitable block pools found to scan.  Waiting 1814399982 ms.
15:47:49.124 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.start(DirectoryScanner.java:282)] - Periodic Directory Tree Verification scan starting at 21-6-1 下午6:30 with interval of 21600000ms
15:47:49.128 [INFO] [BP-1624480315-192.168.131.243-1622533626623 heartbeating to localhost/127.0.0.1:50389] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:767)] - Block pool BP-1624480315-192.168.131.243-1622533626623 (Datanode Uuid fe73600d-f62f-429d-b6fd-810885d31849) service to localhost/127.0.0.1:50389 beginning handshake with NN
15:47:49.136 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1042)] - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50390, datanodeUuid=fe73600d-f62f-429d-b6fd-810885d31849, infoPort=50464, infoSecurePort=0, ipcPort=50468, storageInfo=lv=-57;cid=testClusterID;nsid=999251445;c=1622533626623) storage fe73600d-f62f-429d-b6fd-810885d31849
15:47:49.137 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:145)] - Adding a new node: /default-rack/127.0.0.1:50390
15:47:49.138 [INFO] [IPC Server handler 0 on default port 50389] [org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.registerNode(BlockReportLeaseManager.java:204)] - Registered DN fe73600d-f62f-429d-b6fd-810885d31849 (127.0.0.1:50390).
15:47:49.140 [INFO] [BP-1624480315-192.168.131.243-1622533626623 heartbeating to localhost/127.0.0.1:50389] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:791)] - Block pool Block pool BP-1624480315-192.168.131.243-1622533626623 (Datanode Uuid fe73600d-f62f-429d-b6fd-810885d31849) service to localhost/127.0.0.1:50389 successfully registered with NN
15:47:49.141 [INFO] [BP-1624480315-192.168.131.243-1622533626623 heartbeating to localhost/127.0.0.1:50389] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:616)] - For namenode localhost/127.0.0.1:50389 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
15:47:49.150 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e for DN 127.0.0.1:50390
15:47:49.151 [INFO] [IPC Server handler 6 on default port 50389] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755 for DN 127.0.0.1:50390
15:47:49.167 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x234e74b4526c077c: Processing first storage report for DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755 from datanode fe73600d-f62f-429d-b6fd-810885d31849
15:47:49.168 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x234e74b4526c077c: from storage DS-2222ba9f-dc62-4fc5-90f1-61a1961d2755 node DatanodeRegistration(127.0.0.1:50390, datanodeUuid=fe73600d-f62f-429d-b6fd-810885d31849, infoPort=50464, infoSecurePort=0, ipcPort=50468, storageInfo=lv=-57;cid=testClusterID;nsid=999251445;c=1622533626623), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
15:47:49.168 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x234e74b4526c077c: Processing first storage report for DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e from datanode fe73600d-f62f-429d-b6fd-810885d31849
15:47:49.168 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x234e74b4526c077c: from storage DS-9b70a974-3691-4960-a2f3-5ae73bad0d4e node DatanodeRegistration(127.0.0.1:50390, datanodeUuid=fe73600d-f62f-429d-b6fd-810885d31849, infoPort=50464, infoSecurePort=0, ipcPort=50468, storageInfo=lv=-57;cid=testClusterID;nsid=999251445;c=1622533626623), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
15:47:49.182 [INFO] [BP-1624480315-192.168.131.243-1622533626623 heartbeating to localhost/127.0.0.1:50389] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:424)] - Successfully sent block report 0x234e74b4526c077c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
15:47:49.183 [INFO] [BP-1624480315-192.168.131.243-1622533626623 heartbeating to localhost/127.0.0.1:50389] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:761)] - Got finalize command for block pool BP-1624480315-192.168.131.243-1622533626623
15:47:49.203 [INFO] [IPC Server handler 5 on default port 50389] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:47:49.208 [INFO] [Listener at localhost/50468] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2784)] - Cluster is active
15:47:49.215 [INFO] [Listener at localhost/50468] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - //data/abc/logs
15:47:49.234 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
15:47:49.235 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
15:56:48.877 [INFO] [main] [org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)] - starting cluster: numNameNodes=1, numDataNodes=1
15:56:49.400 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
15:56:49.420 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
15:56:49.421 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
15:56:49.422 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
15:56:49.440 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
15:56:49.440 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
15:56:49.441 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
15:56:49.441 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
15:56:49.501 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:56:49.504 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
15:56:49.505 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
15:56:49.505 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
15:56:49.509 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
15:56:49.510 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 15:56:49
15:56:49.512 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
15:56:49.512 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:56:49.514 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
15:56:49.514 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
15:56:49.535 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
15:56:49.536 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
15:56:49.541 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
15:56:49.542 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
15:56:49.542 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
15:56:49.542 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
15:56:49.543 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
15:56:49.543 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
15:56:49.543 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
15:56:49.543 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
15:56:49.543 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
15:56:49.544 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
15:56:49.544 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
15:56:49.566 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GLOBAL serial map: bits=29 maxEntries=536870911
15:56:49.566 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - USER serial map: bits=24 maxEntries=16777215
15:56:49.566 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GROUP serial map: bits=24 maxEntries=16777215
15:56:49.567 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - XATTR serial map: bits=24 maxEntries=16777215
15:56:49.576 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
15:56:49.576 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:56:49.577 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
15:56:49.577 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
15:56:49.596 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
15:56:49.597 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
15:56:49.597 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
15:56:49.597 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
15:56:49.602 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
15:56:49.604 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
15:56:49.608 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
15:56:49.608 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:56:49.609 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
15:56:49.609 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
15:56:49.617 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
15:56:49.617 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
15:56:49.617 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
15:56:49.621 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
15:56:49.621 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
15:56:49.623 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
15:56:49.623 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:56:49.623 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
15:56:49.624 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
15:56:59.663 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:185)] - Allocated new BlockPoolId: BP-1293821591-192.168.131.243-1622534219651
15:56:59.676 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 has been successfully formatted.
15:56:59.678 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 has been successfully formatted.
15:56:59.702 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
15:56:59.702 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
15:56:59.791 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
15:56:59.791 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
15:56:59.800 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.getImageTxIdToRetain(NNStorageRetentionManager.java:203)] - Going to retain 1 images with txid >= 0
15:56:59.803 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1656)] - createNameNode []
15:57:04.991 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:378)] - Scheduled Metric snapshot period at 10 second(s).
15:57:04.992 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:191)] - NameNode metrics system started
15:57:05.023 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeUtils.getClientNamenodeAddress(NameNodeUtils.java:79)] - fs.defaultFS is hdfs://127.0.0.1:0
15:57:05.060 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4bbf6d0e] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
15:57:05.082 [INFO] [main] [org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1645)] - Starting Web-server for hdfs at: http://localhost:0
15:57:05.099 [INFO] [main] [org.eclipse.jetty.util.log.Log.initialized(Log.java:169)] - Logging initialized @16758ms to org.eclipse.jetty.util.log.Slf4jLog
15:57:10.205 [INFO] [main] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
15:57:10.212 [INFO] [main] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.namenode is not defined
15:57:10.218 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
15:57:10.219 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
15:57:10.220 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
15:57:10.255 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:103)] - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
15:57:10.256 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:819)] - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
15:57:10.271 [INFO] [main] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 54614
15:57:10.273 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
15:57:10.297 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
15:57:10.297 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
15:57:10.298 [INFO] [main] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 660000ms
15:57:10.307 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@2f67b837{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
15:57:10.468 [INFO] [main] [org.eclipse.jetty.util.TypeUtil.<clinit>(TypeUtil.java:201)] - JVM Runtime does not support Modules
15:57:10.477 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@6b8d96d9{hdfs,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-54614-_-any-7197197531965338190.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/hdfs}
15:57:10.486 [INFO] [main] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@2bd08376{HTTP/1.1,[http/1.1]}{localhost:54614}
15:57:10.487 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @22145ms
15:57:10.494 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
15:57:10.502 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
15:57:10.502 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
15:57:10.502 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
15:57:10.503 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
15:57:10.503 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
15:57:10.503 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
15:57:10.503 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
15:57:10.503 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:57:10.504 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
15:57:10.504 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
15:57:10.504 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
15:57:10.504 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 15:57:10
15:57:10.504 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
15:57:10.504 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:57:10.505 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
15:57:10.505 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
15:57:10.519 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
15:57:10.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
15:57:10.520 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
15:57:10.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
15:57:10.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
15:57:10.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
15:57:10.520 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
15:57:10.521 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
15:57:10.521 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
15:57:10.521 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:57:10.522 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
15:57:10.522 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
15:57:10.523 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
15:57:10.523 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
15:57:10.523 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
15:57:10.523 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
15:57:10.523 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
15:57:10.524 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
15:57:10.524 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
15:57:10.524 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:57:10.524 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
15:57:10.524 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
15:57:10.553 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
15:57:10.553 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
15:57:10.554 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
15:57:10.554 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
15:57:10.554 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
15:57:10.555 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
15:57:10.555 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
15:57:10.555 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
15:57:10.555 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
15:57:15.570 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/in_use.lock acquired by nodename 1405@4paradigmdeMacBook-Pro.local
15:57:20.578 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/in_use.lock acquired by nodename 1405@4paradigmdeMacBook-Pro.local
15:57:20.584 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current
15:57:20.585 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current
15:57:20.586 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:733)] - No edit log streams selected.
15:57:20.586 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:799)] - Planning to load image: FSImageFile(file=/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
15:57:20.611 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:234)] - Loading 1 INodes.
15:57:20.616 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:246)] - Loaded FSImage in 0 seconds.
15:57:20.617 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:977)] - Loaded image for txid 0 from /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000
15:57:20.620 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1140)] - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
15:57:20.621 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1365)] - Starting log segment at 1
15:57:20.632 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameCache.initialized(NameCache.java:143)] - initialized with 0 entries 0 lookups
15:57:20.632 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:757)] - Finished loading FSImage in 10075 msecs
15:57:20.790 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:451)] - RPC server is binding to localhost:0
15:57:20.790 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:456)] - Enable NameNode state context:false
15:57:20.797 [INFO] [main] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
15:57:20.807 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
15:57:21.010 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:746)] - Clients are to use localhost:54698 to access this namenode/service.
15:57:21.012 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(FSNamesystem.java:5123)] - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
15:57:21.029 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:176)] - Number of blocks under construction: 0
15:57:21.040 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.initializeReplQueues(BlockManager.java:5069)] - initializing replication queues
15:57:21.041 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:400)] - STATE* Leaving safe mode after 0 secs
15:57:21.041 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:406)] - STATE* Network topology has 0 racks and 0 datanodes
15:57:21.041 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:408)] - STATE* UnderReplicatedBlocks has 0 blocks
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3657)] - Total number of blocks            = 0
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3658)] - Number of invalid blocks          = 0
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3659)] - Number of under-replicated blocks = 0
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3660)] - Number of  over-replicated blocks = 0
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3662)] - Number of blocks being written    = 0
15:57:21.049 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3665)] - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 9 msec
15:57:21.073 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
15:57:21.073 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
15:57:21.075 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:852)] - NameNode RPC up at: localhost/127.0.0.1:54698
15:57:21.077 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1252)] - Starting services required for active state
15:57:21.077 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:799)] - Initializing quota with 4 thread(s)
15:57:21.081 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:808)] - Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
15:57:21.097 [INFO] [CacheReplicationMonitor(612518521)] [org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:160)] - Starting CacheReplicationMonitor with interval 30000 milliseconds
15:57:21.103 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1682)] - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1,[DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:57:21.138 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:57:21.146 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:57:21.163 [INFO] [Listener at localhost/54698] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:158)] - DataNode metrics system started (again)
15:57:21.167 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:57:21.169 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.BlockScanner.<init>(BlockScanner.java:195)] - Initialized block scanner with targetBytesPerSec 1048576
15:57:21.172 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:500)] - Configured hostname is 127.0.0.1
15:57:21.173 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
15:57:21.274 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1403)] - Starting DataNode with maxLockedMemory = 0
15:57:21.279 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:1151)] - Opened streaming server at /127.0.0.1:54699
15:57:21.281 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:78)] - Balancing bandwidth is 10485760 bytes/s
15:57:21.281 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:79)] - Number threads for balancing is 50
15:57:26.290 [INFO] [Listener at localhost/54698] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
15:57:26.291 [INFO] [Listener at localhost/54698] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.datanode is not defined
15:57:26.293 [INFO] [Listener at localhost/54698] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
15:57:26.294 [INFO] [Listener at localhost/54698] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
15:57:26.295 [INFO] [Listener at localhost/54698] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
15:57:26.298 [INFO] [Listener at localhost/54698] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 54733
15:57:26.298 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
15:57:26.299 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
15:57:26.299 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
15:57:26.299 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 600000ms
15:57:26.300 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@181d7f28{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
15:57:26.409 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@55b5e331{datanode,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-54733-_-any-8103497405555206636.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/datanode}
15:57:26.409 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@6a078481{HTTP/1.1,[http/1.1]}{localhost:54733}
15:57:26.410 [INFO] [Listener at localhost/54698] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @38068ms
15:57:31.531 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.start(DatanodeHttpServer.java:256)] - Listening HTTP traffic on /127.0.0.1:54773
15:57:31.532 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@69cac930] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
15:57:31.533 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1431)] - dnUserName = 4paradigm
15:57:31.533 [INFO] [Listener at localhost/54698] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1432)] - supergroup = supergroup
15:57:31.549 [INFO] [Listener at localhost/54698] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
15:57:31.549 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
15:57:31.553 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:1037)] - Opened IPC server at /127.0.0.1:54774
15:57:31.562 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.refreshNamenodes(BlockPoolManager.java:149)] - Refresh request received for nameservices: null
15:57:31.563 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(BlockPoolManager.java:210)] - Starting BPOfferServices for nameservices: <default>
15:57:31.572 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)] - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54698 starting to offer service
15:57:31.576 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
15:57:31.576 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
15:57:31.713 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:379)] - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54698
15:57:31.714 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.getParallelVolumeLoadThreadsNum(DataStorage.java:354)] - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
15:57:31.946 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:31.951 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:31.951 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.056 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.057 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.057 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.162 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.163 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.163 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.269 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.270 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.271 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.376 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.377 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.377 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.482 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.483 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.484 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.591 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.592 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.592 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.699 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.701 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.701 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.807 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.808 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.808 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:32.915 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:32.916 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:32.916 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.018 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.019 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.019 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.123 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.124 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.124 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.230 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.230 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.231 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.338 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.338 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.339 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.441 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.443 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.443 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.550 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.551 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.551 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.653 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.653 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.654 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.755 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.756 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.756 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.859 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.860 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.860 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:33.967 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:33.968 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:33.968 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.074 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.075 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.075 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.181 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.182 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.182 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.284 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.284 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.284 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.387 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.388 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.388 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.496 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.497 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.497 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.600 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.601 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.601 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.703 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.704 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.704 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.807 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.808 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.808 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:34.913 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:34.914 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:34.914 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.018 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.019 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.020 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.125 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.126 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.126 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.232 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.233 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.234 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.338 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.339 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.339 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.445 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.446 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.446 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.553 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.553 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.553 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.659 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.660 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.660 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.767 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.768 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.768 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.874 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.875 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.876 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:35.982 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:35.983 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:35.983 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.090 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.091 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.091 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.196 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.197 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.197 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.300 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.301 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.301 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.406 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.407 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.408 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.514 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.515 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.515 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.620 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.621 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.621 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.717 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/in_use.lock acquired by nodename 1405@4paradigmdeMacBook-Pro.local
15:57:36.719 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 is not formatted for namespace 639108434. Formatting...
15:57:36.720 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-8e2b5f50-7184-4fdb-850c-6e777943124f for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 
15:57:36.726 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.727 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.727 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.833 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.833 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.834 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:36.937 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:36.938 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:36.938 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.040 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.041 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.041 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.147 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.148 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.149 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.253 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.254 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.254 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.358 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.359 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.359 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.464 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.465 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.465 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.569 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.570 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.570 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.676 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.676 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.677 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.783 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.784 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.784 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.886 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.886 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.886 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:37.991 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:37.992 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:37.993 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.097 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.098 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.098 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.202 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.203 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.203 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.305 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.306 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.306 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.412 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.413 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.414 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.518 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.519 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.519 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.626 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.627 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.627 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.732 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.733 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.733 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.838 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.839 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.839 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:38.945 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:38.945 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:38.946 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.052 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.053 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.053 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.159 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.159 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.160 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.265 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.266 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.266 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.372 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.373 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.373 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.476 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.477 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.477 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.580 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.580 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.580 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.682 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.683 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.683 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.789 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.789 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.790 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:39.896 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:39.897 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:39.897 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.001 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.002 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.002 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.108 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.109 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.109 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.210 [INFO] [IPC Server handler 1 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.211 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.211 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.317 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.318 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.318 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.423 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.424 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.424 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.525 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.526 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.527 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.628 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.629 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.630 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.735 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.735 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.736 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.839 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.840 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.840 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:40.946 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:40.947 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:40.947 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.052 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.052 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.053 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.157 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.158 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.159 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.263 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.264 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.264 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.370 [INFO] [IPC Server handler 8 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.371 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.371 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.475 [INFO] [IPC Server handler 9 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.475 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.475 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.580 [INFO] [IPC Server handler 0 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.580 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.580 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.686 [INFO] [IPC Server handler 7 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.687 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.687 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.726 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/in_use.lock acquired by nodename 1405@4paradigmdeMacBook-Pro.local
15:57:41.726 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 is not formatted for namespace 639108434. Formatting...
15:57:41.727 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4 for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 
15:57:41.751 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1293821591-192.168.131.243-1622534219651
15:57:41.752 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1293821591-192.168.131.243-1622534219651
15:57:41.753 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 and block pool id BP-1293821591-192.168.131.243-1622534219651 is not formatted. Formatting ...
15:57:41.753 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1293821591-192.168.131.243-1622534219651 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1293821591-192.168.131.243-1622534219651/current
15:57:41.771 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1293821591-192.168.131.243-1622534219651
15:57:41.771 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1293821591-192.168.131.243-1622534219651
15:57:41.772 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 and block pool id BP-1293821591-192.168.131.243-1622534219651 is not formatted. Formatting ...
15:57:41.772 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1293821591-192.168.131.243-1622534219651 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1293821591-192.168.131.243-1622534219651/current
15:57:41.773 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1751)] - Setting up storage: nsid=639108434;bpid=BP-1293821591-192.168.131.243-1622534219651;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=639108434;c=1622534219651;bpid=BP-1293821591-192.168.131.243-1622534219651;dnuuid=null
15:57:41.774 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.checkDatanodeUuid(DataNode.java:1549)] - Generated and persisted new Datanode UUID e6064858-fec2-47eb-a69e-2cff5cfc47fc
15:57:41.790 [INFO] [IPC Server handler 2 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.791 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.791 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.855 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-8e2b5f50-7184-4fdb-850c-6e777943124f
15:57:41.855 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, StorageType: DISK
15:57:41.856 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4
15:57:41.857 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, StorageType: DISK
15:57:41.860 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader.initialize(MemoryMappableBlockLoader.java:48)] - Initializing cache loader: MemoryMappableBlockLoader.
15:57:41.862 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.registerMBean(FsDatasetImpl.java:2320)] - Registered FSDatasetState MBean
15:57:41.866 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:2833)] - Adding block pool BP-1293821591-192.168.131.243-1622534219651
15:57:41.867 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
15:57:41.867 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
15:57:41.897 [INFO] [IPC Server handler 4 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:41.897 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.shouldWait(MiniDFSCluster.java:2808)] - dnInfo.length != numDataNodes
15:57:41.897 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2729)] - Waiting for cluster to become active
15:57:41.905 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1293821591-192.168.131.243-1622534219651 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 38ms
15:57:41.905 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1293821591-192.168.131.243-1622534219651 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 38ms
15:57:41.905 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:431)] - Total time to scan all replicas for block pool BP-1293821591-192.168.131.243-1622534219651: 39ms
15:57:41.906 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
15:57:41.906 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
15:57:41.907 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1293821591-192.168.131.243-1622534219651/current/replicas doesn't exist 
15:57:41.907 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1293821591-192.168.131.243-1622534219651/current/replicas doesn't exist 
15:57:41.907 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 1ms
15:57:41.907 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 1ms
15:57:41.908 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getAllVolumesMap(FsVolumeList.java:225)] - Total time to add all replicas to map for block pool BP-1293821591-192.168.131.243-1622534219651: 1ms
15:57:41.908 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:57:41.913 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:57:41.914 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:57:41.914 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:57:41.915 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
15:57:41.915 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1293821591-192.168.131.243-1622534219651 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
15:57:41.919 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4): finished scanning block pool BP-1293821591-192.168.131.243-1622534219651
15:57:41.919 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-8e2b5f50-7184-4fdb-850c-6e777943124f): finished scanning block pool BP-1293821591-192.168.131.243-1622534219651
15:57:41.930 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-8e2b5f50-7184-4fdb-850c-6e777943124f): no suitable block pools found to scan.  Waiting 1814399985 ms.
15:57:41.931 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4): no suitable block pools found to scan.  Waiting 1814399984 ms.
15:57:41.932 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.start(DirectoryScanner.java:282)] - Periodic Directory Tree Verification scan starting at 21-6-1 下午7:39 with interval of 21600000ms
15:57:41.936 [INFO] [BP-1293821591-192.168.131.243-1622534219651 heartbeating to localhost/127.0.0.1:54698] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:767)] - Block pool BP-1293821591-192.168.131.243-1622534219651 (Datanode Uuid e6064858-fec2-47eb-a69e-2cff5cfc47fc) service to localhost/127.0.0.1:54698 beginning handshake with NN
15:57:41.944 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1042)] - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:54699, datanodeUuid=e6064858-fec2-47eb-a69e-2cff5cfc47fc, infoPort=54773, infoSecurePort=0, ipcPort=54774, storageInfo=lv=-57;cid=testClusterID;nsid=639108434;c=1622534219651) storage e6064858-fec2-47eb-a69e-2cff5cfc47fc
15:57:41.946 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:145)] - Adding a new node: /default-rack/127.0.0.1:54699
15:57:41.946 [INFO] [IPC Server handler 6 on default port 54698] [org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.registerNode(BlockReportLeaseManager.java:204)] - Registered DN e6064858-fec2-47eb-a69e-2cff5cfc47fc (127.0.0.1:54699).
15:57:41.949 [INFO] [BP-1293821591-192.168.131.243-1622534219651 heartbeating to localhost/127.0.0.1:54698] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:791)] - Block pool Block pool BP-1293821591-192.168.131.243-1622534219651 (Datanode Uuid e6064858-fec2-47eb-a69e-2cff5cfc47fc) service to localhost/127.0.0.1:54698 successfully registered with NN
15:57:41.949 [INFO] [BP-1293821591-192.168.131.243-1622534219651 heartbeating to localhost/127.0.0.1:54698] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:616)] - For namenode localhost/127.0.0.1:54698 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
15:57:41.960 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-8e2b5f50-7184-4fdb-850c-6e777943124f for DN 127.0.0.1:54699
15:57:41.961 [INFO] [IPC Server handler 5 on default port 54698] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4 for DN 127.0.0.1:54699
15:57:41.977 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x45dccff18a347353: Processing first storage report for DS-8e2b5f50-7184-4fdb-850c-6e777943124f from datanode e6064858-fec2-47eb-a69e-2cff5cfc47fc
15:57:41.978 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x45dccff18a347353: from storage DS-8e2b5f50-7184-4fdb-850c-6e777943124f node DatanodeRegistration(127.0.0.1:54699, datanodeUuid=e6064858-fec2-47eb-a69e-2cff5cfc47fc, infoPort=54773, infoSecurePort=0, ipcPort=54774, storageInfo=lv=-57;cid=testClusterID;nsid=639108434;c=1622534219651), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
15:57:41.979 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x45dccff18a347353: Processing first storage report for DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4 from datanode e6064858-fec2-47eb-a69e-2cff5cfc47fc
15:57:41.979 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x45dccff18a347353: from storage DS-1b193390-39b2-46c5-b31e-ec2a3a86b3f4 node DatanodeRegistration(127.0.0.1:54699, datanodeUuid=e6064858-fec2-47eb-a69e-2cff5cfc47fc, infoPort=54773, infoSecurePort=0, ipcPort=54774, storageInfo=lv=-57;cid=testClusterID;nsid=639108434;c=1622534219651), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
15:57:41.992 [INFO] [BP-1293821591-192.168.131.243-1622534219651 heartbeating to localhost/127.0.0.1:54698] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:424)] - Successfully sent block report 0x45dccff18a347353,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
15:57:41.993 [INFO] [BP-1293821591-192.168.131.243-1622534219651 heartbeating to localhost/127.0.0.1:54698] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:761)] - Got finalize command for block pool BP-1293821591-192.168.131.243-1622534219651
15:57:41.999 [INFO] [IPC Server handler 3 on default port 54698] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
15:57:42.004 [INFO] [Listener at localhost/54774] [org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2784)] - Cluster is active
15:57:42.010 [INFO] [Listener at localhost/54774] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - /data/abc/logs
15:57:42.021 [INFO] [Listener at localhost/54774] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:48)] - hdfs://localhost:54698
15:57:42.025 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
15:57:42.026 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:08:21.250 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
16:08:21.269 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
16:08:21.270 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
16:08:21.271 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
16:08:21.285 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
16:08:21.285 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
16:08:21.285 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
16:08:21.286 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
16:08:21.337 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
16:08:21.340 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
16:08:21.341 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
16:08:21.341 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
16:08:21.345 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
16:08:21.346 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 16:08:21
16:08:21.348 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
16:08:21.349 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:21.351 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
16:08:21.351 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
16:08:21.375 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
16:08:21.375 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
16:08:21.386 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
16:08:21.387 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
16:08:21.387 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
16:08:21.387 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
16:08:21.388 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
16:08:21.388 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
16:08:21.388 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
16:08:21.389 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
16:08:21.389 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
16:08:21.389 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
16:08:21.389 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
16:08:21.416 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GLOBAL serial map: bits=29 maxEntries=536870911
16:08:21.416 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - USER serial map: bits=24 maxEntries=16777215
16:08:21.416 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - GROUP serial map: bits=24 maxEntries=16777215
16:08:21.416 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)] - XATTR serial map: bits=24 maxEntries=16777215
16:08:21.431 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
16:08:21.431 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:21.432 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
16:08:21.432 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
16:08:21.452 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
16:08:21.452 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
16:08:21.453 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
16:08:21.453 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
16:08:21.458 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
16:08:21.459 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
16:08:21.464 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
16:08:21.464 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:21.464 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
16:08:21.465 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
16:08:21.472 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
16:08:21.472 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
16:08:21.472 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
16:08:21.476 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
16:08:21.476 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
16:08:21.479 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
16:08:21.479 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:21.479 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
16:08:21.479 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
16:08:31.518 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:185)] - Allocated new BlockPoolId: BP-1839600621-192.168.131.243-1622534911502
16:08:31.531 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 has been successfully formatted.
16:08:31.533 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorage.format(NNStorage.java:595)] - Storage directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 has been successfully formatted.
16:08:31.558 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
16:08:31.558 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:512)] - Saving image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
16:08:31.646 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
16:08:31.646 [INFO] [FSImageSaver for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2 of type IMAGE_AND_EDITS] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver.save(FSImageFormatProtobuf.java:516)] - Image file /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .
16:08:31.654 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.getImageTxIdToRetain(NNStorageRetentionManager.java:203)] - Going to retain 1 images with txid >= 0
16:08:31.656 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1656)] - createNameNode []
16:08:36.857 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:378)] - Scheduled Metric snapshot period at 10 second(s).
16:08:36.858 [INFO] [main] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:191)] - NameNode metrics system started
16:08:36.885 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeUtils.getClientNamenodeAddress(NameNodeUtils.java:79)] - fs.defaultFS is hdfs://127.0.0.1:0
16:08:36.917 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@791d1f8b] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
16:08:36.937 [INFO] [main] [org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1645)] - Starting Web-server for hdfs at: http://localhost:0
16:08:36.953 [INFO] [main] [org.eclipse.jetty.util.log.Log.initialized(Log.java:169)] - Logging initialized @16833ms to org.eclipse.jetty.util.log.Slf4jLog
16:08:42.063 [INFO] [main] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
16:08:42.069 [INFO] [main] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.namenode is not defined
16:08:42.079 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
16:08:42.081 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
16:08:42.082 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
16:08:42.117 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:103)] - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
16:08:42.121 [INFO] [main] [org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:819)] - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
16:08:42.151 [INFO] [main] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 59717
16:08:42.153 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
16:08:42.201 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
16:08:42.201 [INFO] [main] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
16:08:42.204 [INFO] [main] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 660000ms
16:08:42.224 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@6af9fcb2{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
16:08:42.432 [INFO] [main] [org.eclipse.jetty.util.TypeUtil.<clinit>(TypeUtil.java:201)] - JVM Runtime does not support Modules
16:08:42.442 [INFO] [main] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@758705fa{hdfs,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-59717-_-any-7963940403287365313.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/hdfs}
16:08:42.453 [INFO] [main] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@3d3e5463{HTTP/1.1,[http/1.1]}{localhost:59717}
16:08:42.453 [INFO] [main] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @22333ms
16:08:42.461 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229)] - Edit logging is async:true
16:08:42.469 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:785)] - KeyProvider: null
16:08:42.469 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:123)] - fsLock is fair: true
16:08:42.469 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141)] - Detailed lock hold time metrics enabled: false
16:08:42.469 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:810)] - fsOwner             = 4paradigm (auth:SIMPLE)
16:08:42.470 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:811)] - supergroup          = supergroup
16:08:42.470 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:812)] - isPermissionEnabled = true
16:08:42.470 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:823)] - HA Enabled: false
16:08:42.470 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
16:08:42.471 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:303)] - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
16:08:42.471 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:311)] - dfs.namenode.datanode.registration.ip-hostname-check=true
16:08:42.471 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:79)] - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
16:08:42.471 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:85)] - The block deletion will start around 2021 六月 01 16:08:42
16:08:42.472 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map BlocksMap
16:08:42.472 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:42.472 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 2.0% max memory 3.6 GB = 72.8 MB
16:08:42.472 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^23 = 8388608 entries
16:08:42.491 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5330)] - Storage policy satisfier is disabled
16:08:42.491 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:624)] - dfs.block.access.token.enable = false
16:08:42.492 [INFO] [main] [org.apache.hadoop.conf.Configuration.logDeprecation(Configuration.java:1395)] - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
16:08:42.492 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161)] - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
16:08:42.492 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:162)] - dfs.namenode.safemode.min.datanodes = 0
16:08:42.494 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:164)] - dfs.namenode.safemode.extension = 0
16:08:42.494 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:610)] - defaultReplication         = 1
16:08:42.494 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:611)] - maxReplication             = 512
16:08:42.494 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:612)] - minReplication             = 1
16:08:42.495 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613)] - maxReplicationStreams      = 2
16:08:42.495 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:614)] - redundancyRecheckInterval  = 3000ms
16:08:42.495 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:615)] - encryptDataTransfer        = false
16:08:42.495 [INFO] [main] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:616)] - maxNumBlocksToLog          = 1000
16:08:42.496 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map INodeMap
16:08:42.496 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:42.496 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 1.0% max memory 3.6 GB = 36.4 MB
16:08:42.496 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^22 = 4194304 entries
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:292)] - ACLs enabled? false
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:296)] - POSIX ACL inheritance enabled? true
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:300)] - XAttrs enabled? true
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:364)] - Caching file names occurring more than 10 times
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.<init>(SnapshotManager.java:124)] - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
16:08:42.498 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory.init(DirectoryDiffListFactory.java:43)] - SkipList is disabled
16:08:42.499 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map cachedBlocks
16:08:42.499 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:42.499 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.25% max memory 3.6 GB = 9.1 MB
16:08:42.499 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^20 = 1048576 entries
16:08:42.531 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:75)] - NNTop conf: dfs.namenode.top.window.num.buckets = 10
16:08:42.532 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:77)] - NNTop conf: dfs.namenode.top.num.users = 10
16:08:42.532 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.logConf(TopMetrics.java:79)] - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
16:08:42.532 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1026)] - Retry cache on namenode is enabled
16:08:42.532 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initRetryCache(FSNamesystem.java:1034)] - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
16:08:42.532 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395)] - Computing capacity for map NameNodeRetryCache
16:08:42.533 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396)] - VM type       = 64-bit
16:08:42.533 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397)] - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
16:08:42.533 [INFO] [main] [org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402)] - capacity      = 2^17 = 131072 entries
16:08:47.546 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/in_use.lock acquired by nodename 2181@4paradigmdeMacBook-Pro.local
16:08:52.553 [INFO] [main] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/in_use.lock acquired by nodename 2181@4paradigmdeMacBook-Pro.local
16:08:52.556 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current
16:08:52.556 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428)] - Recovering unfinalized segments in /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-2/current
16:08:52.558 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:733)] - No edit log streams selected.
16:08:52.558 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:799)] - Planning to load image: FSImageFile(file=/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
16:08:52.581 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:234)] - Loading 1 INodes.
16:08:52.587 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:246)] - Loaded FSImage in 0 seconds.
16:08:52.587 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:977)] - Loaded image for txid 0 from /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/name-0-1/current/fsimage_0000000000000000000
16:08:52.591 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1140)] - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
16:08:52.592 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1365)] - Starting log segment at 1
16:08:52.609 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameCache.initialized(NameCache.java:143)] - initialized with 0 entries 0 lookups
16:08:52.609 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:757)] - Finished loading FSImage in 10075 msecs
16:08:52.773 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:451)] - RPC server is binding to localhost:0
16:08:52.774 [INFO] [main] [org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:456)] - Enable NameNode state context:false
16:08:52.783 [INFO] [main] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
16:08:52.794 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
16:08:53.064 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:746)] - Clients are to use localhost:59809 to access this namenode/service.
16:08:53.067 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(FSNamesystem.java:5123)] - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
16:08:53.099 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:176)] - Number of blocks under construction: 0
16:08:53.107 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.initializeReplQueues(BlockManager.java:5069)] - initializing replication queues
16:08:53.108 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:400)] - STATE* Leaving safe mode after 0 secs
16:08:53.108 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:406)] - STATE* Network topology has 0 racks and 0 datanodes
16:08:53.108 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:408)] - STATE* UnderReplicatedBlocks has 0 blocks
16:08:53.115 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3657)] - Total number of blocks            = 0
16:08:53.116 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3658)] - Number of invalid blocks          = 0
16:08:53.116 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3659)] - Number of under-replicated blocks = 0
16:08:53.116 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3660)] - Number of  over-replicated blocks = 0
16:08:53.116 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3662)] - Number of blocks being written    = 0
16:08:53.116 [INFO] [Reconstruction Queue Initializer] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processMisReplicatesAsync(BlockManager.java:3665)] - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
16:08:53.143 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
16:08:53.143 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
16:08:53.145 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:852)] - NameNode RPC up at: localhost/127.0.0.1:59809
16:08:53.147 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1252)] - Starting services required for active state
16:08:53.147 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:799)] - Initializing quota with 4 thread(s)
16:08:53.152 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:808)] - Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
16:08:53.169 [INFO] [CacheReplicationMonitor(532868555)] [org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:160)] - Starting CacheReplicationMonitor with interval 30000 milliseconds
16:08:53.221 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
16:08:53.230 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
16:08:53.246 [INFO] [Listener at localhost/59809] [org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:158)] - DataNode metrics system started (again)
16:08:53.252 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
16:08:53.255 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.BlockScanner.<init>(BlockScanner.java:195)] - Initialized block scanner with targetBytesPerSec 1048576
16:08:53.258 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:500)] - Configured hostname is 127.0.0.1
16:08:53.259 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395)] - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
16:08:53.362 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1403)] - Starting DataNode with maxLockedMemory = 0
16:08:53.367 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:1151)] - Opened streaming server at /127.0.0.1:59813
16:08:53.369 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:78)] - Balancing bandwidth is 10485760 bytes/s
16:08:53.370 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler.<init>(DataXceiverServer.java:79)] - Number threads for balancing is 50
16:08:58.380 [INFO] [Listener at localhost/59809] [org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240)] - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
16:08:58.381 [INFO] [Listener at localhost/59809] [org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82)] - Http request log for http.requests.datanode is not defined
16:08:58.382 [INFO] [Listener at localhost/59809] [org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1003)] - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
16:08:58.384 [INFO] [Listener at localhost/59809] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:976)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
16:08:58.384 [INFO] [Listener at localhost/59809] [org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:986)] - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
16:08:58.387 [INFO] [Listener at localhost/59809] [org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1219)] - Jetty bound to port 59853
16:08:58.388 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.Server.doStart(Server.java:359)] - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_282-b08
16:08:58.389 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:333)] - DefaultSessionIdManager workerName=node0
16:08:58.389 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.session.DefaultSessionIdManager.doStart(DefaultSessionIdManager.java:338)] - No SessionScavenger set, using defaults
16:08:58.389 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.session.HouseKeeper.startScavenging(HouseKeeper.java:140)] - node0 Scavenging every 600000ms
16:08:58.390 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.s.ServletContextHandler@2fb69ff6{static,/static,jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/static,AVAILABLE}
16:08:58.494 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:824)] - Started o.e.j.w.WebAppContext@15fc442{datanode,/,file:///private/var/folders/7_/1nqs7pwj47l9zg80sslz5p1w0000gn/T/jetty-localhost-59853-_-any-3128551020532081405.dir/webapp/,AVAILABLE}{jar:file:/Users/4paradigm/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2-tests.jar!/webapps/datanode}
16:08:58.494 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:293)] - Started ServerConnector@1929425f{HTTP/1.1,[http/1.1]}{localhost:59853}
16:08:58.495 [INFO] [Listener at localhost/59809] [org.eclipse.jetty.server.Server.doStart(Server.java:399)] - Started @38374ms
16:09:03.676 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.start(DatanodeHttpServer.java:256)] - Listening HTTP traffic on /127.0.0.1:59891
16:09:03.677 [INFO] [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5d39f2d8] [org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:188)] - Starting JVM pause monitor
16:09:03.678 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1431)] - dnUserName = 4paradigm
16:09:03.678 [INFO] [Listener at localhost/59809] [org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1432)] - supergroup = supergroup
16:09:03.689 [INFO] [Listener at localhost/59809] [org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:85)] - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
16:09:03.689 [INFO] [Socket Reader #1 for port 0] [org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1255)] - Starting Socket Reader #1 for port 0
16:09:03.694 [INFO] [Listener at localhost/59892] [org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:1037)] - Opened IPC server at /127.0.0.1:59892
16:09:03.703 [INFO] [Listener at localhost/59892] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.refreshNamenodes(BlockPoolManager.java:149)] - Refresh request received for nameservices: null
16:09:03.705 [INFO] [Listener at localhost/59892] [org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(BlockPoolManager.java:210)] - Starting BPOfferServices for nameservices: <default>
16:09:03.714 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)] - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:59809 starting to offer service
16:09:03.718 [INFO] [IPC Server Responder] [org.apache.hadoop.ipc.Server$Responder.run(Server.java:1497)] - IPC Server Responder: starting
16:09:03.718 [INFO] [IPC Server listener on 0] [org.apache.hadoop.ipc.Server$Listener.run(Server.java:1334)] - IPC Server listener on 0: starting
16:09:03.876 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:379)] - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:59809
16:09:03.878 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.getParallelVolumeLoadThreadsNum(DataStorage.java:354)] - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
16:09:04.048 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.159 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.265 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.371 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.477 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.580 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.683 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.790 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.893 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:04.999 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.104 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.211 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.315 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.416 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.523 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.630 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.736 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.841 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:05.949 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.056 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.162 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.263 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.368 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.476 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.583 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.689 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.792 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.896 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:06.999 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.107 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.214 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.320 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.427 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.531 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.638 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.745 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.847 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:07.952 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.059 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.162 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.264 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.371 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.477 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.584 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.690 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.793 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:08.882 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/in_use.lock acquired by nodename 2181@4paradigmdeMacBook-Pro.local
16:09:08.883 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 is not formatted for namespace 1513983576. Formatting...
16:09:08.885 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47 for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 
16:09:08.898 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.002 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.108 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.213 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.320 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.425 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.528 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.634 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.738 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.844 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:09.950 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.056 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.160 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.266 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.371 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.476 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.582 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.689 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.792 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:10.898 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.004 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.109 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.211 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.315 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.418 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.522 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.626 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.731 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.833 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:11.938 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.043 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.147 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.253 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.359 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.463 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.565 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.670 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.777 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.882 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:12.989 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.095 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.199 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.303 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.408 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.511 [INFO] [IPC Server handler 2 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.615 [INFO] [IPC Server handler 9 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.719 [INFO] [IPC Server handler 3 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.823 [INFO] [IPC Server handler 4 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.889 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:928)] - Lock on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/in_use.lock acquired by nodename 2181@4paradigmdeMacBook-Pro.local
16:09:13.890 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:282)] - Storage directory with location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 is not formatted for namespace 1513983576. Formatting...
16:09:13.890 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataStorage.createStorageID(DataStorage.java:160)] - Generated new storageID DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71 for directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 
16:09:13.918 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1839600621-192.168.131.243-1622534911502
16:09:13.918 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1839600621-192.168.131.243-1622534911502
16:09:13.919 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1 and block pool id BP-1839600621-192.168.131.243-1622534911502 is not formatted. Formatting ...
16:09:13.919 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1839600621-192.168.131.243-1622534911502 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1839600621-192.168.131.243-1622534911502/current
16:09:13.929 [INFO] [IPC Server handler 0 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:13.940 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:251)] - Analyzing storage directories for bpid BP-1839600621-192.168.131.243-1622534911502
16:09:13.941 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:887)] - Locking is disabled for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1839600621-192.168.131.243-1622534911502
16:09:13.941 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:168)] - Block pool storage directory for location [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2 and block pool id BP-1839600621-192.168.131.243-1622534911502 is not formatted. Formatting ...
16:09:13.941 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.format(BlockPoolSliceStorage.java:280)] - Formatting block pool BP-1839600621-192.168.131.243-1622534911502 directory /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1839600621-192.168.131.243-1622534911502/current
16:09:13.942 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1751)] - Setting up storage: nsid=1513983576;bpid=BP-1839600621-192.168.131.243-1622534911502;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1513983576;c=1622534911502;bpid=BP-1839600621-192.168.131.243-1622534911502;dnuuid=null
16:09:13.943 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DataNode.checkDatanodeUuid(DataNode.java:1549)] - Generated and persisted new Datanode UUID 1048a975-ece6-4890-b75c-cebee61bc458
16:09:14.031 [INFO] [IPC Server handler 1 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:14.041 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47
16:09:14.041 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, StorageType: DISK
16:09:14.042 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:304)] - Added new volume: DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71
16:09:14.043 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:464)] - Added volume - [DISK]file:/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, StorageType: DISK
16:09:14.046 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader.initialize(MemoryMappableBlockLoader.java:48)] - Initializing cache loader: MemoryMappableBlockLoader.
16:09:14.049 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.registerMBean(FsDatasetImpl.java:2320)] - Registered FSDatasetState MBean
16:09:14.053 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:2833)] - Adding block pool BP-1839600621-192.168.131.243-1622534911502
16:09:14.054 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
16:09:14.054 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:406)] - Scanning block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
16:09:14.094 [INFO] [Thread-79] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1839600621-192.168.131.243-1622534911502 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 39ms
16:09:14.094 [INFO] [Thread-78] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:411)] - Time taken to scan block pool BP-1839600621-192.168.131.243-1622534911502 on /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 39ms
16:09:14.094 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:431)] - Total time to scan all replicas for block pool BP-1839600621-192.168.131.243-1622534911502: 41ms
16:09:14.095 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1...
16:09:14.095 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:199)] - Adding replicas to map for block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2...
16:09:14.095 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1/current/BP-1839600621-192.168.131.243-1622534911502/current/replicas doesn't exist 
16:09:14.095 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(BlockPoolSlice.java:881)] - Replica Cache file: /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2/current/BP-1839600621-192.168.131.243-1622534911502/current/replicas doesn't exist 
16:09:14.096 [INFO] [Thread-83] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2: 1ms
16:09:14.096 [INFO] [Thread-82] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:204)] - Time to add replicas to map for block pool BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1: 1ms
16:09:14.097 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getAllVolumesMap(FsVolumeList.java:225)] - Total time to add all replicas to map for block pool BP-1839600621-192.168.131.243-1622534911502: 1ms
16:09:14.097 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
16:09:14.102 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
16:09:14.103 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:137)] - Scheduling a check for /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
16:09:14.103 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:222)] - Scheduled health check for volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
16:09:14.105 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2
16:09:14.105 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:381)] - Now scanning bpid BP-1839600621-192.168.131.243-1622534911502 on volume /Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1
16:09:14.108 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71): finished scanning block pool BP-1839600621-192.168.131.243-1622534911502
16:09:14.109 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47): finished scanning block pool BP-1839600621-192.168.131.243-1622534911502
16:09:14.121 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data1, DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47): no suitable block pools found to scan.  Waiting 1814399984 ms.
16:09:14.121 [INFO] [VolumeScannerThread(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2)] [org.apache.hadoop.hdfs.server.datanode.VolumeScanner.findNextUsableBlockIter(VolumeScanner.java:398)] - VolumeScanner(/Users/4paradigm/Docker/centos6/fedb/src/tools/dataImporter/target/hdfs/minicluster/data/data2, DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71): no suitable block pools found to scan.  Waiting 1814399984 ms.
16:09:14.123 [INFO] [Thread-59] [org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.start(DirectoryScanner.java:282)] - Periodic Directory Tree Verification scan starting at 21-6-1 下午5:47 with interval of 21600000ms
16:09:14.128 [INFO] [BP-1839600621-192.168.131.243-1622534911502 heartbeating to localhost/127.0.0.1:59809] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:767)] - Block pool BP-1839600621-192.168.131.243-1622534911502 (Datanode Uuid 1048a975-ece6-4890-b75c-cebee61bc458) service to localhost/127.0.0.1:59809 beginning handshake with NN
16:09:14.137 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1042)] - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:59813, datanodeUuid=1048a975-ece6-4890-b75c-cebee61bc458, infoPort=59891, infoSecurePort=0, ipcPort=59892, storageInfo=lv=-57;cid=testClusterID;nsid=1513983576;c=1622534911502) storage 1048a975-ece6-4890-b75c-cebee61bc458
16:09:14.138 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:145)] - Adding a new node: /default-rack/127.0.0.1:59813
16:09:14.139 [INFO] [IPC Server handler 5 on default port 59809] [org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.registerNode(BlockReportLeaseManager.java:204)] - Registered DN 1048a975-ece6-4890-b75c-cebee61bc458 (127.0.0.1:59813).
16:09:14.141 [INFO] [IPC Server handler 7 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:14.142 [INFO] [BP-1839600621-192.168.131.243-1622534911502 heartbeating to localhost/127.0.0.1:59809] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:791)] - Block pool Block pool BP-1839600621-192.168.131.243-1622534911502 (Datanode Uuid 1048a975-ece6-4890-b75c-cebee61bc458) service to localhost/127.0.0.1:59809 successfully registered with NN
16:09:14.142 [INFO] [BP-1839600621-192.168.131.243-1622534911502 heartbeating to localhost/127.0.0.1:59809] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:616)] - For namenode localhost/127.0.0.1:59809 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
16:09:14.153 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47 for DN 127.0.0.1:59813
16:09:14.153 [INFO] [IPC Server handler 6 on default port 59809] [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorage(DatanodeDescriptor.java:993)] - Adding new storage ID DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71 for DN 127.0.0.1:59813
16:09:14.174 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x9af3e801904b80fa: Processing first storage report for DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47 from datanode 1048a975-ece6-4890-b75c-cebee61bc458
16:09:14.175 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x9af3e801904b80fa: from storage DS-f25688b7-cc55-42f1-93ec-3b5cf3202a47 node DatanodeRegistration(127.0.0.1:59813, datanodeUuid=1048a975-ece6-4890-b75c-cebee61bc458, infoPort=59891, infoSecurePort=0, ipcPort=59892, storageInfo=lv=-57;cid=testClusterID;nsid=1513983576;c=1622534911502), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
16:09:14.175 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x9af3e801904b80fa: Processing first storage report for DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71 from datanode 1048a975-ece6-4890-b75c-cebee61bc458
16:09:14.175 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x9af3e801904b80fa: from storage DS-c7988be7-d15e-4d21-bf1e-0db5474b9b71 node DatanodeRegistration(127.0.0.1:59813, datanodeUuid=1048a975-ece6-4890-b75c-cebee61bc458, infoPort=59891, infoSecurePort=0, ipcPort=59892, storageInfo=lv=-57;cid=testClusterID;nsid=1513983576;c=1622534911502), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
16:09:14.192 [INFO] [BP-1839600621-192.168.131.243-1622534911502 heartbeating to localhost/127.0.0.1:59809] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:424)] - Successfully sent block report 0x9af3e801904b80fa,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 24 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
16:09:14.192 [INFO] [BP-1839600621-192.168.131.243-1622534911502 heartbeating to localhost/127.0.0.1:59809] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:761)] - Got finalize command for block pool BP-1839600621-192.168.131.243-1622534911502
16:09:14.248 [INFO] [IPC Server handler 8 on default port 59809] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
16:09:14.253 [INFO] [Listener at localhost/59892] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:41)] - dfs is on: hdfs://localhost:59809
16:09:14.256 [INFO] [Listener at localhost/59892] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - /data/abc/logs
16:09:14.268 [INFO] [Listener at localhost/59892] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:51)] - hdfs://localhost:59809
16:09:14.273 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:09:14.273 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:18:50.795 [INFO] [Listener at localhost/64325] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:41)] - dfs is on: hdfs://localhost:64246
16:18:50.798 [INFO] [Listener at localhost/64325] [com._4paradigm.dataimporter.WildcardURI.<init>(WildcardURI.java:41)] - /data/abc/logs
16:18:50.812 [INFO] [Listener at localhost/64325] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:51)] - hdfs://localhost:64246
16:18:50.816 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:18:50.817 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:37:03.751 [INFO] [Listener at localhost/55820] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:44)] - dfs is on: hdfs://localhost:55743
16:37:03.771 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:37:03.772 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:38:58.630 [INFO] [Listener at localhost/56672] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:44)] - dfs is on: hdfs://localhost:56586
16:38:58.664 [INFO] [IPC Server handler 9 on default port 56586] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
16:38:58.666 [INFO] [Listener at localhost/56672] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:61)] - hdfs://localhost:56586
16:38:58.670 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:38:58.671 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:41:41.133 [INFO] [Listener at localhost/57844] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:45)] - dfs is on: hdfs://localhost:57767
16:41:41.146 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x96025e59273aa721: Processing first storage report for DS-785e6701-dbb1-473f-b703-3c07fbcd0484 from datanode ee71fa1a-8dc3-4d28-be46-6e34c38341af
16:41:41.148 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x96025e59273aa721: from storage DS-785e6701-dbb1-473f-b703-3c07fbcd0484 node DatanodeRegistration(127.0.0.1:57768, datanodeUuid=ee71fa1a-8dc3-4d28-be46-6e34c38341af, infoPort=57843, infoSecurePort=0, ipcPort=57844, storageInfo=lv=-57;cid=testClusterID;nsid=557117513;c=1622536858983), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
16:41:41.148 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2718)] - BLOCK* processReport 0x96025e59273aa721: Processing first storage report for DS-027106ad-291e-4ce1-a846-4098fc06ff2a from datanode ee71fa1a-8dc3-4d28-be46-6e34c38341af
16:41:41.149 [INFO] [Block report processor] [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2747)] - BLOCK* processReport 0x96025e59273aa721: from storage DS-027106ad-291e-4ce1-a846-4098fc06ff2a node DatanodeRegistration(127.0.0.1:57768, datanodeUuid=ee71fa1a-8dc3-4d28-be46-6e34c38341af, infoPort=57843, infoSecurePort=0, ipcPort=57844, storageInfo=lv=-57;cid=testClusterID;nsid=557117513;c=1622536858983), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
16:41:41.168 [INFO] [BP-1097760458-192.168.131.243-1622536858983 heartbeating to localhost/127.0.0.1:57767] [org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:424)] - Successfully sent block report 0x96025e59273aa721,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 34 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
16:41:41.169 [INFO] [BP-1097760458-192.168.131.243-1622536858983 heartbeating to localhost/127.0.0.1:57767] [org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:761)] - Got finalize command for block pool BP-1097760458-192.168.131.243-1622536858983
16:41:41.177 [INFO] [IPC Server handler 2 on default port 57767] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
16:41:41.179 [INFO] [Listener at localhost/57844] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:62)] - hdfs://localhost:57767
16:41:41.183 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:41:41.183 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:46:27.095 [INFO] [Listener at localhost/59952] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:45)] - dfs is on: hdfs://localhost:59870
16:46:27.133 [INFO] [IPC Server handler 1 on default port 59870] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
16:46:27.140 [INFO] [IPC Server handler 4 on default port 59870] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
16:46:27.143 [INFO] [Listener at localhost/59952] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:63)] - [Lorg.apache.hadoop.fs.FileStatus;@1a480135
16:46:27.147 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:46:27.147 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:59:09.912 [INFO] [Listener at localhost/65422] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:46)] - dfs is on: hdfs://localhost:65318
16:59:09.947 [INFO] [IPC Server handler 6 on default port 65318] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
16:59:09.952 [INFO] [IPC Server handler 2 on default port 65318] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
16:59:09.954 [INFO] [Listener at localhost/65422] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:64)] - [Lorg.apache.hadoop.fs.FileStatus;@1a480135
16:59:09.958 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
16:59:09.958 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:02:35.009 [INFO] [Listener at localhost/50567] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:46)] - dfs is on: hdfs://localhost:50488
17:02:35.042 [INFO] [IPC Server handler 7 on default port 50488] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
17:02:35.048 [INFO] [IPC Server handler 4 on default port 50488] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
17:02:35.049 [INFO] [Listener at localhost/50567] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:64)] - [Lorg.apache.hadoop.fs.FileStatus;@1a480135
17:02:35.054 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:02:35.054 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:40:22.023 [INFO] [Listener at localhost/50771] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:54)] - dfs is on: hdfs://localhost:50687
17:40:22.059 [INFO] [IPC Server handler 4 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
17:40:22.064 [INFO] [IPC Server handler 0 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
17:40:22.065 [INFO] [Listener at localhost/50771] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:72)] - [Lorg.apache.hadoop.fs.FileStatus;@6e8a9c30
17:40:22.089 [INFO] [IPC Server handler 2 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/data1.csv	dst=null	perm=4paradigm:supergroup:rw-r--r--	proto=rpc
17:40:22.119 [INFO] [IPC Server handler 8 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.logAllocatedBlock(FSDirWriteFileOp.java:798)] - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50691 for /data1.csv
17:40:22.182 [INFO] [DataXceiver for client DFSClient_NONMAPREDUCE_-1012640443_1 at /127.0.0.1:50847 [Receiving block BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001]] [org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:747)] - Receiving BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001 src: /127.0.0.1:50847 dest: /127.0.0.1:50691
17:40:22.226 [INFO] [PacketResponder: BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1533)] - src: /127.0.0.1:50847, dest: /127.0.0.1:50691, bytes: 172, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1012640443_1, offset: 0, srvID: 714cdc53-cba1-4d07-946b-2cb498048281, blockid: BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001, duration(ns): 5832239
17:40:22.226 [INFO] [PacketResponder: BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1506)] - PacketResponder: BP-842314028-192.168.131.243-1622540379660:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
17:40:22.231 [INFO] [IPC Server handler 5 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkBlocksComplete(FSNamesystem.java:3014)] - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data1.csv
17:40:22.644 [INFO] [IPC Server handler 3 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2967)] - DIR* completeFile: /data1.csv is closed by DFSClient_NONMAPREDUCE_-1012640443_1
17:40:22.650 [INFO] [IPC Server handler 6 on default port 50687] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/data1.csv	dst=null	perm=null	proto=rpc
17:40:22.652 [INFO] [Listener at localhost/50771] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:92)] - HdfsNamedFileStatus{path=hdfs://localhost:50687/data1.csv; isDirectory=false; length=172; replication=3; blocksize=134217728; modification_time=1622540422643; access_time=1622540422081; owner=4paradigm; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
17:40:22.656 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:40:22.657 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:47:27.880 [INFO] [Listener at localhost/53796] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:57)] - dfs is on: hdfs://localhost:53701
17:47:27.913 [INFO] [IPC Server handler 9 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
17:47:27.917 [INFO] [IPC Server handler 7 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
17:47:27.919 [INFO] [Listener at localhost/53796] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:75)] - [Lorg.apache.hadoop.fs.FileStatus;@6e8a9c30
17:47:27.937 [INFO] [IPC Server handler 0 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/data1.csv	dst=null	perm=4paradigm:supergroup:rw-r--r--	proto=rpc
17:47:27.963 [INFO] [IPC Server handler 5 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.logAllocatedBlock(FSDirWriteFileOp.java:798)] - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:53705 for /data1.csv
17:47:28.010 [INFO] [DataXceiver for client DFSClient_NONMAPREDUCE_-1099440655_1 at /127.0.0.1:53897 [Receiving block BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001]] [org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:747)] - Receiving BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001 src: /127.0.0.1:53897 dest: /127.0.0.1:53705
17:47:28.044 [INFO] [PacketResponder: BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1533)] - src: /127.0.0.1:53897, dest: /127.0.0.1:53705, bytes: 172, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1099440655_1, offset: 0, srvID: 93d36240-f5e8-4b05-9ff3-0ccc12a76b73, blockid: BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001, duration(ns): 6938798
17:47:28.045 [INFO] [PacketResponder: BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1506)] - PacketResponder: BP-868543133-192.168.131.243-1622540805247:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
17:47:28.051 [INFO] [IPC Server handler 3 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkBlocksComplete(FSNamesystem.java:3014)] - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data1.csv
17:47:28.457 [INFO] [IPC Server handler 8 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2967)] - DIR* completeFile: /data1.csv is closed by DFSClient_NONMAPREDUCE_-1099440655_1
17:47:28.460 [INFO] [IPC Server handler 2 on default port 53701] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/data1.csv	dst=null	perm=null	proto=rpc
17:47:28.461 [INFO] [Listener at localhost/53796] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:95)] - HdfsNamedFileStatus{path=hdfs://localhost:53701/data1.csv; isDirectory=false; length=172; replication=3; blocksize=134217728; modification_time=1622540848456; access_time=1622540847929; owner=4paradigm; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
17:47:28.465 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:47:28.466 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:52:15.594 [INFO] [Listener at localhost/55918] [com._4paradigm.dataimporter.FileSystemManagerTest.setUp(FileSystemManagerTest.java:57)] - dfs is on: hdfs://localhost:55838
17:52:15.630 [INFO] [IPC Server handler 4 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/test_dir	dst=null	perm=4paradigm:supergroup:rwxr-xr-x	proto=rpc
17:52:15.635 [INFO] [IPC Server handler 5 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/test_dir	dst=null	perm=null	proto=rpc
17:52:15.637 [INFO] [Listener at localhost/55918] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:75)] - [Lorg.apache.hadoop.fs.FileStatus;@6c1832aa
17:52:15.657 [INFO] [IPC Server handler 8 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/data1.csv	dst=null	perm=4paradigm:supergroup:rw-r--r--	proto=rpc
17:52:15.686 [INFO] [IPC Server handler 6 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.logAllocatedBlock(FSDirWriteFileOp.java:798)] - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:55845 for /data1.csv
17:52:15.740 [INFO] [DataXceiver for client DFSClient_NONMAPREDUCE_-1471530616_1 at /127.0.0.1:55992 [Receiving block BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001]] [org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:747)] - Receiving BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001 src: /127.0.0.1:55992 dest: /127.0.0.1:55845
17:52:15.772 [INFO] [PacketResponder: BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1533)] - src: /127.0.0.1:55992, dest: /127.0.0.1:55845, bytes: 172, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1471530616_1, offset: 0, srvID: 4aaa27d3-544e-4f26-91f5-340a9c1f80e6, blockid: BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001, duration(ns): 7281305
17:52:15.773 [INFO] [PacketResponder: BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001, type=LAST_IN_PIPELINE] [org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1506)] - PacketResponder: BP-2041602707-192.168.131.243-1622541093292:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
17:52:15.778 [INFO] [IPC Server handler 2 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkBlocksComplete(FSNamesystem.java:3014)] - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /data1.csv
17:52:16.184 [INFO] [IPC Server handler 1 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2967)] - DIR* completeFile: /data1.csv is closed by DFSClient_NONMAPREDUCE_-1471530616_1
17:52:16.187 [INFO] [IPC Server handler 9 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/data1.csv	dst=null	perm=null	proto=rpc
17:52:16.189 [INFO] [Listener at localhost/55918] [com._4paradigm.dataimporter.FileSystemManagerTest.testGetFileSystemSuccess(FileSystemManagerTest.java:94)] - HdfsNamedFileStatus{path=hdfs://localhost:55838/data1.csv; isDirectory=false; length=172; replication=3; blocksize=134217728; modification_time=1622541136183; access_time=1622541135648; owner=4paradigm; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
17:52:16.196 [INFO] [IPC Server handler 0 on default port 55838] [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8145)] - allowed=true	ugi=4paradigm (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/data1.csv	dst=null	perm=null	proto=rpc
17:52:16.251 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
17:52:16.252 [INFO] [shutdown-hook-0] [org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.lambda$run$0(FSImage.java:1053)] - FSImageSaver clean checkpoint: txid=0 when meet shutdown.
